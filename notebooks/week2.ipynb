{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Ingest and Explore the Dataset\n",
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dask[complete]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is too large to be loaded at once, we will be use the chunksize function in pandas to initially explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>ult_fec_cli_1t</th>\n",
       "      <th>indrel_1mes</th>\n",
       "      <th>tiprel_1mes</th>\n",
       "      <th>indresi</th>\n",
       "      <th>indext</th>\n",
       "      <th>conyuemp</th>\n",
       "      <th>canal_entrada</th>\n",
       "      <th>indfall</th>\n",
       "      <th>tipodom</th>\n",
       "      <th>cod_prov</th>\n",
       "      <th>nomprov</th>\n",
       "      <th>ind_actividad_cliente</th>\n",
       "      <th>renta</th>\n",
       "      <th>segmento</th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>ind_cco_fin_ult1</th>\n",
       "      <th>ind_cder_fin_ult1</th>\n",
       "      <th>ind_cno_fin_ult1</th>\n",
       "      <th>ind_ctju_fin_ult1</th>\n",
       "      <th>ind_ctma_fin_ult1</th>\n",
       "      <th>ind_ctop_fin_ult1</th>\n",
       "      <th>ind_ctpp_fin_ult1</th>\n",
       "      <th>ind_deco_fin_ult1</th>\n",
       "      <th>ind_deme_fin_ult1</th>\n",
       "      <th>ind_dela_fin_ult1</th>\n",
       "      <th>ind_ecue_fin_ult1</th>\n",
       "      <th>ind_fond_fin_ult1</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1375586</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>35</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHL</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>MALAGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87218.10</td>\n",
       "      <td>02 - PARTICULARES</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050611</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>23</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHE</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>CIUDAD REAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35548.74</td>\n",
       "      <td>03 - UNIVERSITARIO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050612</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>23</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHE</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>CIUDAD REAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122179.11</td>\n",
       "      <td>03 - UNIVERSITARIO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050613</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>22</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHD</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ZARAGOZA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119775.54</td>\n",
       "      <td>03 - UNIVERSITARIO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050614</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>23</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHE</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ZARAGOZA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03 - UNIVERSITARIO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fecha_dato  ncodpers ind_empleado pais_residencia sexo  age  fecha_alta  \\\n",
       "0  2015-01-28   1375586            N              ES    H   35  2015-01-12   \n",
       "1  2015-01-28   1050611            N              ES    V   23  2012-08-10   \n",
       "2  2015-01-28   1050612            N              ES    V   23  2012-08-10   \n",
       "3  2015-01-28   1050613            N              ES    H   22  2012-08-10   \n",
       "4  2015-01-28   1050614            N              ES    V   23  2012-08-10   \n",
       "\n",
       "   ind_nuevo antiguedad  indrel ult_fec_cli_1t  indrel_1mes tiprel_1mes  \\\n",
       "0        0.0          6     1.0            NaN          1.0           A   \n",
       "1        0.0         35     1.0            NaN          1.0           I   \n",
       "2        0.0         35     1.0            NaN          1.0           I   \n",
       "3        0.0         35     1.0            NaN          1.0           I   \n",
       "4        0.0         35     1.0            NaN          1.0           A   \n",
       "\n",
       "  indresi indext  conyuemp canal_entrada indfall  tipodom  cod_prov  \\\n",
       "0       S      N       NaN           KHL       N      1.0      29.0   \n",
       "1       S      S       NaN           KHE       N      1.0      13.0   \n",
       "2       S      N       NaN           KHE       N      1.0      13.0   \n",
       "3       S      N       NaN           KHD       N      1.0      50.0   \n",
       "4       S      N       NaN           KHE       N      1.0      50.0   \n",
       "\n",
       "       nomprov  ind_actividad_cliente      renta            segmento  \\\n",
       "0       MALAGA                    1.0   87218.10   02 - PARTICULARES   \n",
       "1  CIUDAD REAL                    0.0   35548.74  03 - UNIVERSITARIO   \n",
       "2  CIUDAD REAL                    0.0  122179.11  03 - UNIVERSITARIO   \n",
       "3     ZARAGOZA                    0.0  119775.54  03 - UNIVERSITARIO   \n",
       "4     ZARAGOZA                    1.0        NaN  03 - UNIVERSITARIO   \n",
       "\n",
       "   ind_ahor_fin_ult1  ind_aval_fin_ult1  ind_cco_fin_ult1  ind_cder_fin_ult1  \\\n",
       "0                  0                  0                 1                  0   \n",
       "1                  0                  0                 1                  0   \n",
       "2                  0                  0                 1                  0   \n",
       "3                  0                  0                 0                  0   \n",
       "4                  0                  0                 1                  0   \n",
       "\n",
       "   ind_cno_fin_ult1  ind_ctju_fin_ult1  ind_ctma_fin_ult1  ind_ctop_fin_ult1  \\\n",
       "0                 0                  0                  0                  0   \n",
       "1                 0                  0                  0                  0   \n",
       "2                 0                  0                  0                  0   \n",
       "3                 0                  0                  0                  0   \n",
       "4                 0                  0                  0                  0   \n",
       "\n",
       "   ind_ctpp_fin_ult1  ind_deco_fin_ult1  ind_deme_fin_ult1  ind_dela_fin_ult1  \\\n",
       "0                  0                  0                  0                  0   \n",
       "1                  0                  0                  0                  0   \n",
       "2                  0                  0                  0                  0   \n",
       "3                  0                  1                  0                  0   \n",
       "4                  0                  0                  0                  0   \n",
       "\n",
       "   ind_ecue_fin_ult1  ind_fond_fin_ult1  ind_hip_fin_ult1  ind_plan_fin_ult1  \\\n",
       "0                  0                  0                 0                  0   \n",
       "1                  0                  0                 0                  0   \n",
       "2                  0                  0                 0                  0   \n",
       "3                  0                  0                 0                  0   \n",
       "4                  0                  0                 0                  0   \n",
       "\n",
       "   ind_pres_fin_ult1  ind_reca_fin_ult1  ind_tjcr_fin_ult1  ind_valo_fin_ult1  \\\n",
       "0                  0                  0                  0                  0   \n",
       "1                  0                  0                  0                  0   \n",
       "2                  0                  0                  0                  0   \n",
       "3                  0                  0                  0                  0   \n",
       "4                  0                  0                  0                  0   \n",
       "\n",
       "   ind_viv_fin_ult1  ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "0                 0              0.0                0.0                0  \n",
       "1                 0              0.0                0.0                0  \n",
       "2                 0              0.0                0.0                0  \n",
       "3                 0              0.0                0.0                0  \n",
       "4                 0              0.0                0.0                0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('/home/jupyter-fagundem/applied_analytics_project/data/raw/train_ver2.csv', chunksize = chunk_size)\n",
    "first_chunk = next(chunks)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "first_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output allows us to identify how the columns are organized and some examples of the cells we will see in the dataset. With this, we can start working on the data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date = fecha_dato   2015-01-28\n",
      "dtype: datetime64[ns] \n",
      " Last date = fecha_dato   2016-05-28\n",
      "dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "col = ['fecha_dato']\n",
    "dates = pd.read_csv('/home/jupyter-fagundem/applied_analytics_project/data/raw/train_ver2.csv', usecols=col)\n",
    "dates['fecha_dato'] = pd.to_datetime(dates['fecha_dato'], errors='coerce')\n",
    "\n",
    "old = dates.min()\n",
    "new = dates.max()\n",
    "\n",
    "print(f'First date = {old} \\n Last date = {new}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato  fecha_dato\n",
       "2015        1             625457\n",
       "            2             627394\n",
       "            3             629209\n",
       "            4             630367\n",
       "            5             631957\n",
       "            6             632110\n",
       "            7             829817\n",
       "            8             843201\n",
       "            9             865440\n",
       "            10            892251\n",
       "            11            906109\n",
       "            12            912021\n",
       "2016        1             916269\n",
       "            2             920904\n",
       "            3             925076\n",
       "            4             928274\n",
       "            5             931453\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_count = dates.groupby([dates['fecha_dato'].dt.year, dates['fecha_dato'].dt.month]).size()\n",
    "monthly_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the full dataset is over 2BG, we can't upload it in Jupyter, hence here we are exploring the dates range to determine where it will be our cuttoff. \n",
    "The data seems to be well distributed along the months so our cuttoff will be on June of 2016 and our final train dataset will have one year worth of records "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is too large to ingest at once, we will use Dask dataframes to process the initial changes. Dask handles datasets larger than the available memory by partitioning the data and processing it in parallel across multiple processors or machines -it works like a pandas dataframe, but with parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will export the data as objects so we don't get any dtypes errors for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.read_csv('/home/jupyter-fagundem/applied_analytics_project/data/raw/train_ver2.csv', assume_missing=True, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fecha_dato'] = dd.to_datetime(data['fecha_dato'], errors='coerce')\n",
    "\n",
    "cutoff = pd.Timestamp('2015-06-01')\n",
    "\n",
    "filtered_data = data[data['fecha_dato'] >= cutoff]\n",
    "\n",
    "rename_col = {\n",
    "    'fecha_dato': 'date',\n",
    "    'ncodpers': 'customer_code',\n",
    "    'ind_empleado': 'employee_index',\n",
    "    'pais_residencia': 'country',\n",
    "    'sexo': 'sex_H',\n",
    "    'age': 'age',\n",
    "    'fecha_alta': 'first_contract_date',\n",
    "    'ind_nuevo': 'new_cust',\n",
    "    'antiguedad': 'seniority_in_months',\n",
    "    'indrel': 'primary_cust',\n",
    "    'ult_fec_cli_1t': 'last_date_primary',\n",
    "    'indrel_1mes': 'cust_type',\n",
    "    'tiprel_1mes': 'cust_relationship',\n",
    "    'indresi': 'residency_spain',\n",
    "    'indext': 'birth_spain',\n",
    "    'conyuemp': 'employee_spouse',\n",
    "    'canal_entrada': 'join_channel',\n",
    "    'indfall': 'deceased',\n",
    "    'tipodom': 'address_type',\n",
    "    'cod_prov': 'province_code',\n",
    "    'nomprov': 'province_name',\n",
    "    'ind_actividad_cliente': 'active_cust',\n",
    "    'renta': 'income',\n",
    "    'segmento': 'segment',\n",
    "    'ind_ahor_fin_ult1': 'savings_acct',\n",
    "    'ind_aval_fin_ult1': 'guarantees',\n",
    "    'ind_cco_fin_ult1': 'current_acct',\n",
    "    'ind_cder_fin_ult1': 'derivada_acct',\n",
    "    'ind_cno_fin_ult1': 'payroll_acct',\n",
    "    'ind_ctju_fin_ult1': 'junior_acct',\n",
    "    'ind_ctma_fin_ult1': 'mas_particular_acct',\n",
    "    'ind_ctop_fin_ult1': 'particular_acct',\n",
    "    'ind_ctpp_fin_ult1': 'particular_plus_acct',\n",
    "    'ind_deco_fin_ult1': 'short_term_depo',\n",
    "    'ind_deme_fin_ult1': 'medium_term_depo',\n",
    "    'ind_dela_fin_ult1': 'long_term_depo',\n",
    "    'ind_ecue_fin_ult1': 'e_acct',\n",
    "    'ind_fond_fin_ult1': 'funds',\n",
    "    'ind_hip_fin_ult1': 'mortgage',\n",
    "    'ind_plan_fin_ult1': 'pension',\n",
    "    'ind_pres_fin_ult1': 'loans',\n",
    "    'ind_reca_fin_ult1': 'taxes',\n",
    "    'ind_tjcr_fin_ult1': 'credit_card',\n",
    "    'ind_valo_fin_ult1': 'securities',\n",
    "    'ind_viv_fin_ult1': 'home_acct',\n",
    "    'ind_nomina_ult1': 'payroll_acct',\n",
    "    'ind_nom_pens_ult1': 'pensions_2',\n",
    "    'ind_recibo_ult1': 'direct_debt'\n",
    "}\n",
    "\n",
    "filtered_data = filtered_data.rename(columns=rename_col)\n",
    "na_check = filtered_data.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are filtering out the early months of the dataset and changing to columns name from Spanish to English for best comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call compute(). Since dask uses a parallel processing, it performs what is called lazy operation, meaning that the changes are not applied to the whole dataset unless it is forced -by using compute(). We want to force it here so we can start seeing null values and other important characteristics of the dataset to strat the cleaning process, which is what we are doing on the next code by seeing what kind of values are on each column and how many null values each column has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date variables: <DatetimeArray>\n",
      "['2015-06-28 00:00:00', '2015-07-28 00:00:00', '2015-08-28 00:00:00',\n",
      " '2015-09-28 00:00:00', '2015-10-28 00:00:00', '2015-11-28 00:00:00',\n",
      " '2015-12-28 00:00:00', '2016-01-28 00:00:00', '2016-02-28 00:00:00',\n",
      " '2016-03-28 00:00:00', '2016-04-28 00:00:00', '2016-05-28 00:00:00']\n",
      "Length: 12, dtype: datetime64[ns]\n",
      "NA values: 0\n",
      "customer_code variables: ['  16132' '1063040' '1063041' ... '1173729' '1164094' '1550586']\n",
      "NA values: 0\n",
      "employee_index variables: ['N' nan 'A' 'B' 'F' 'S']\n",
      "NA values: 1861\n",
      "country variables: ['ES' nan 'CL' 'NL' 'AT' 'CH' 'CA' 'IE' 'GB' 'AR' 'DE' 'DO' 'BE' 'MX' 'FR'\n",
      " 'VE' 'QA' 'US' 'HN' 'EC' 'CR' 'CO' 'NI' 'BR' 'PT' 'MZ' 'AL' 'SE' 'IT'\n",
      " 'PE' 'IN' 'PY' 'MA' 'PL' 'CN' 'FI' 'TW' 'GR' 'AE' 'PR' 'HK' 'RO' 'GT'\n",
      " 'NO' 'BG' 'GA' 'RU' 'UA' 'SN' 'MR' 'EE' 'SV' 'CZ' 'IL' 'SA' 'CI' 'LU'\n",
      " 'PA' 'ET' 'CM' 'BA' 'BO' 'HR' 'SG' 'BY' 'NG' 'CU' 'JP' 'SK' 'AU' 'MD'\n",
      " 'TR' 'KE' 'UY' 'ZA' 'GE' 'DK' 'AD' 'GQ' 'EG' 'DZ' 'TH' 'PK' 'LY' 'TN'\n",
      " 'TG' 'LB' 'KR' 'KH' 'GH' 'RS' 'KW' 'PH' 'VN' 'AO' 'MM' 'NZ' 'GI' 'LV'\n",
      " 'SL' 'GN' 'GW' 'CG' 'ML' 'HU' 'MK' 'OM' 'LT' 'IS' 'CD' 'GM' 'KZ' 'CF'\n",
      " 'BZ' 'ZW' 'DJ' 'JM' 'BM' 'MT']\n",
      "NA values: 1861\n",
      "sex_H variables: ['V' 'H' nan]\n",
      "NA values: 1918\n",
      "age variables: [' 48' ' 25' ' 24' ' 26' ' 23' ' 22' ' 29' ' 36' ' 32' ' 30' ' 28' ' 56'\n",
      " ' 27' ' 40' ' 34' ' 63' ' 53' ' 39' ' 60' ' 42' ' 31' ' 41' ' NA' ' 45'\n",
      " ' 37' ' 35' ' 57' ' 55' ' 51' ' 58' ' 46' ' 44' ' 50' ' 65' ' 47' ' 75'\n",
      " ' 38' ' 49' ' 43' ' 52' '  5' ' 18' ' 13' ' 11' ' 59' ' 33' ' 70' ' 69'\n",
      " ' 61' ' 82' ' 68' ' 54' ' 12' ' 67' ' 14' ' 71' ' 77' ' 92' '  6' ' 10'\n",
      " '  7' ' 84' ' 73' ' 62' ' 95' ' 17' ' 87' ' 15' ' 72' ' 64' ' 21' ' 66'\n",
      " ' 85' ' 83' ' 16' '  8' ' 20' ' 86' '  9' ' 19' ' 79' ' 74' ' 80' ' 96'\n",
      " ' 81' ' 89' ' 90' ' 78' ' 88' '100' ' 76' ' 91' ' 94' ' 93' ' 98' '  4'\n",
      " ' 97' '104' '106' '101' '103' ' 99' '  3' '  2' '102' '107' '111' '109'\n",
      " '105' '110' '112' '115' '108' '116' '113' '126' '117' '163' '127' '114'\n",
      " '164']\n",
      "NA values: 0\n",
      "first_contract_date variables: ['1995-03-08' '2012-09-19' nan ... '2016-05-25' '2016-05-01' '2016-05-15']\n",
      "NA values: 1861\n",
      "new_cust variables: [' 0' nan ' 1']\n",
      "NA values: 1861\n",
      "seniority_in_months variables: ['    244' '     34' '     NA' '     25' '     33' '     22' '     10'\n",
      " '     21' '      9' '     17' '     12' '     20' '     30' '     18'\n",
      " '      2' '      5' '     24' '      6' '     27' '     19' '      8'\n",
      " '     13' '     32' '      7' '     11' '     28' '     15' '     35'\n",
      " '     16' '     23' '      3' '     26' '     31' '      1' '      4'\n",
      " '     29' '    157' '     14' '     36' '     40' '    139' '     46'\n",
      " '     38' '     45' '     44' '     43' '     41' '     39' '     47'\n",
      " '     42' '     37' '     49' '     50' '     48' '     51' '     56'\n",
      " '     54' '     55' '      0' '     53' '     52' '     57' '     58'\n",
      " '    209' '    165' '    164' '    105' '     81' '    129' '    109'\n",
      " '    128' '    108' '    156' '    121' '    136' '    150' '    142'\n",
      " '     64' '    122' '    125' '    146' '    138' '    101' '     69'\n",
      " '    163' '    116' '     96' '    117' '    107' '    137' '    145'\n",
      " '     61' '    162' '    160' '    102' '     88' '     65' '    114'\n",
      " '    113' '    161' '    217' '     77' '    154' '    152' '    126'\n",
      " '    159' '    166' '    104' '    119' '     94' '    149' '    103'\n",
      " '     82' '     76' '    151' '     70' '     86' '     79' '    135'\n",
      " '    169' '     60' '    118' '    134' '    120' '    110' '    148'\n",
      " '     78' '    141' '     66' '    140' '     99' '    147' '    100'\n",
      " '     95' '    133' '    124' '    127' '    193' '     80' '    132'\n",
      " '     83' '    123' '    231' '    158' '    143' '    187' '    111'\n",
      " '     85' '     98' '    170' '    106' '     84' '     63' '    155'\n",
      " '    189' '    175' '     87' '    177' '    115' '    112' '    232'\n",
      " '     97' '    144' '     93' '    203' '    131' '    172' '    190'\n",
      " '     72' '    176' '    153' '     89' '    174' '    194' '     71'\n",
      " '    173' '    212' '     68' '     59' '     74' '    130' '     73'\n",
      " '    183' '    180' '     62' '    216' '    179' '    178' '    168'\n",
      " '    184' '    171' '    167' '    198' '     92' '    199' '    206'\n",
      " '    235' '    213' '    208' '     75' '    195' '    201' '    186'\n",
      " '     67' '    188' '     90' '    207' '    185' '    192' '    182'\n",
      " '     91' '    215' '    211' '    181' '    196' '    219' '    205'\n",
      " '    202' '    200' '    214' '    191' '    227' '    218' '    225'\n",
      " '    224' '    226' '    242' '    210' '    223' '    237' '    222'\n",
      " '    204' '    233' '    220' '    228' '    197' '    221' '    241'\n",
      " '    229' '    240' '    234' '    243' '    230' '    238' '    246'\n",
      " '    236' '    239' '    245' '-999999' '    247' '    248' '    249'\n",
      " '    250' '    251' '    252' '    253' '    254' '    255' '    256']\n",
      "NA values: 0\n",
      "primary_cust variables: [' 1' nan '99']\n",
      "NA values: 1861\n",
      "last_date_primary variables: [nan '2015-07-13' '2015-07-29' '2015-07-30' '2015-07-23' '2015-07-06'\n",
      " '2015-07-03' '2015-07-01' '2015-07-21' '2015-07-14' '2015-07-10'\n",
      " '2015-07-27' '2015-07-16' '2015-07-15' '2015-07-08' '2015-07-20'\n",
      " '2015-07-07' '2015-07-09' '2015-07-02' '2015-07-28' '2015-07-22'\n",
      " '2015-07-17' '2015-07-24' '2015-08-21' '2015-08-19' '2015-08-25'\n",
      " '2015-08-14' '2015-08-24' '2015-08-17' '2015-08-18' '2015-08-10'\n",
      " '2015-08-13' '2015-08-27' '2015-08-03' '2015-08-06' '2015-08-20'\n",
      " '2015-08-26' '2015-08-28' '2015-08-05' '2015-08-11' '2015-08-07'\n",
      " '2015-08-04' '2015-08-12' '2015-09-17' '2015-09-01' '2015-09-18'\n",
      " '2015-09-03' '2015-09-02' '2015-09-14' '2015-09-16' '2015-09-29'\n",
      " '2015-09-28' '2015-09-09' '2015-09-22' '2015-09-08' '2015-09-11'\n",
      " '2015-09-21' '2015-09-04' '2015-09-25' '2015-09-07' '2015-09-10'\n",
      " '2015-09-23' '2015-09-24' '2015-09-15' '2015-10-08' '2015-10-07'\n",
      " '2015-10-13' '2015-10-26' '2015-10-29' '2015-10-05' '2015-10-28'\n",
      " '2015-10-09' '2015-10-22' '2015-10-20' '2015-10-15' '2015-10-06'\n",
      " '2015-10-01' '2015-10-21' '2015-10-16' '2015-10-27' '2015-10-19'\n",
      " '2015-10-23' '2015-10-02' '2015-10-14' '2015-11-23' '2015-11-24'\n",
      " '2015-11-12' '2015-11-04' '2015-11-13' '2015-11-25' '2015-11-19'\n",
      " '2015-11-20' '2015-11-03' '2015-11-16' '2015-11-17' '2015-11-11'\n",
      " '2015-11-27' '2015-11-18' '2015-11-10' '2015-11-26' '2015-11-02'\n",
      " '2015-11-05' '2015-11-06' '2015-11-09' '2015-12-21' '2015-12-18'\n",
      " '2015-12-28' '2015-12-24' '2015-12-04' '2015-12-29' '2015-12-16'\n",
      " '2015-12-11' '2015-12-30' '2015-12-15' '2015-12-01' '2015-12-09'\n",
      " '2015-12-10' '2015-12-17' '2015-12-02' '2015-12-14' '2015-12-03'\n",
      " '2015-12-22' '2015-12-23' '2015-12-07' '2016-01-08' '2016-01-14'\n",
      " '2016-01-13' '2016-01-28' '2016-01-05' '2016-01-19' '2016-01-12'\n",
      " '2016-01-18' '2016-01-21' '2016-01-22' '2016-01-07' '2016-01-20'\n",
      " '2016-01-26' '2016-01-15' '2016-01-27' '2016-01-25' '2016-01-11'\n",
      " '2016-01-04' '2016-02-23' '2016-02-19' '2016-02-18' '2016-02-26'\n",
      " '2016-02-12' '2016-02-24' '2016-02-09' '2016-02-08' '2016-02-11'\n",
      " '2016-02-05' '2016-02-04' '2016-02-03' '2016-02-15' '2016-02-22'\n",
      " '2016-02-10' '2016-02-16' '2016-02-01' '2016-02-17' '2016-02-02'\n",
      " '2016-02-25' '2016-03-07' '2016-03-29' '2016-03-10' '2016-03-18'\n",
      " '2016-03-14' '2016-03-22' '2016-03-08' '2016-03-21' '2016-03-30'\n",
      " '2016-03-01' '2016-03-23' '2016-03-02' '2016-03-24' '2016-03-03'\n",
      " '2016-03-09' '2016-03-11' '2016-03-04' '2016-03-16' '2016-03-28'\n",
      " '2016-03-15' '2016-03-17' '2016-04-22' '2016-04-01' '2016-04-06'\n",
      " '2016-04-12' '2016-04-05' '2016-04-15' '2016-04-13' '2016-04-19'\n",
      " '2016-04-04' '2016-04-18' '2016-04-26' '2016-04-11' '2016-04-25'\n",
      " '2016-04-27' '2016-04-08' '2016-04-07' '2016-04-21' '2016-04-28'\n",
      " '2016-04-20' '2016-04-14' '2016-05-23' '2016-05-05' '2016-05-17'\n",
      " '2016-05-19' '2016-05-12' '2016-05-06' '2016-05-03' '2016-05-20'\n",
      " '2016-05-02' '2016-05-16' '2016-05-18' '2016-05-04' '2016-05-13'\n",
      " '2016-05-24' '2016-05-27' '2016-05-10' '2016-05-30' '2016-05-25'\n",
      " '2016-05-11' '2016-05-09' '2016-05-26']\n",
      "NA values: 10482071\n",
      "cust_type variables: ['1' nan '1.0' '3.0' '2.0' '3' '4.0' 'P' '4' '2']\n",
      "NA values: 123908\n",
      "cust_relationship variables: ['A' 'I' nan 'P' 'R' 'N']\n",
      "NA values: 123908\n",
      "residency_spain variables: ['S' nan 'N']\n",
      "NA values: 1861\n",
      "birth_spain variables: ['N' 'S' nan]\n",
      "NA values: 1861\n",
      "employee_spouse variables: [nan 'N' 'S']\n",
      "NA values: 10501557\n",
      "join_channel variables: ['KAT' 'KHE' 'KHD' nan 'KFC' 'KFA' 'KHC' 'KAZ' 'KHK' 'KHL' 'KGN' 'RED'\n",
      " 'KHN' 'KDH' 'KEH' 'KGC' 'KHM' 'KHO' 'KHF' 'KFK' 'KHA' 'KAF' 'K00' '013'\n",
      " 'KAR' 'KFJ' 'KAG' 'KAA' 'KFF' 'KAI' 'KCC' 'KFG' 'KFP' 'KFD' 'KGX' 'KAH'\n",
      " 'KAE' 'KFS' 'KAB' 'KFN' 'KAP' 'KFL' 'KFU' 'KGY' 'KAQ' 'KGV' 'KAJ' 'KAD'\n",
      " 'KBG' 'KHQ' 'KAK' '007' 'KDR' 'KCA' 'KDT' 'KBO' 'KBQ' 'KAY' 'KCG' 'KBU'\n",
      " 'KBZ' '004' 'KDO' 'KCK' 'KEC' 'KAC' 'KEU' 'KDE' 'KDY' 'KCH' 'KCI' 'KCL'\n",
      " 'KDA' 'KES' 'KAS' 'KDX' 'KCM' 'KCN' 'KDQ' 'KCB' 'KDU' 'KAL' 'KAW' 'KEY'\n",
      " 'KDZ' 'KCS' 'KCD' 'KCE' 'KEJ' 'KDC' 'KBL' 'KAO' 'KEA' 'KEW' 'KFT' 'KEV'\n",
      " 'KBH' 'KEG' 'KEI' 'KEO' 'KBD' 'KDP' 'KBV' 'KCO' 'KBR' 'KCV' 'KBF' 'KCU'\n",
      " 'KBX' 'KDD' 'KBW' 'KCF' 'KAN' 'KEZ' 'KAM' 'KDS' 'KBY' 'KEF' 'KBS' 'KDF'\n",
      " 'KCP' 'KDB' 'KBP' 'KBE' 'KCT' 'KCX' 'KBN' 'KDV' 'KDG' 'KEB' 'KEL' 'KDW'\n",
      " 'KBB' 'KBJ' 'KDM' 'KFH' 'KBM' 'KEN' 'KFI' 'KEQ' 'KAV' 'KFM' 'KAU' 'KED'\n",
      " 'KEK' 'KFR' 'KFB' 'KFE' 'KGW' 'KFV' 'KGU' 'KDI' 'KEE' 'KCQ' 'KCR' 'KDN'\n",
      " 'KEM' 'KCJ' 'KDL' '025' 'KHP' 'KHR' 'KHS']\n",
      "NA values: 160057\n",
      "deceased variables: ['N' nan 'S']\n",
      "NA values: 1861\n",
      "address_type variables: [' 1' nan]\n",
      "NA values: 1862\n",
      "province_code variables: ['28' '46' '23' '11' '36' '15' '33' '29' '21' '41' ' 2' '12' '14' '50'\n",
      " '27' '30' ' 6' ' 7' '45' '24' ' 3' '25' '18' '32' ' 5' '37' '44' nan ' 8'\n",
      " '39' '10' '43' '34' '35' ' 9' '13' '22' '31' '38' '20' '52' ' 1' '19'\n",
      " '26' '42' ' 4' '17' '47' '16' '49' '48' '51' '40']\n",
      "NA values: 49330\n",
      "province_name variables: ['MADRID' 'VALENCIA' 'JAEN' 'CADIZ' 'PONTEVEDRA' 'CORUÑA, A' 'ASTURIAS'\n",
      " 'MALAGA' 'HUELVA' 'SEVILLA' 'ALBACETE' 'CASTELLON' 'CORDOBA' 'ZARAGOZA'\n",
      " 'LUGO' 'MURCIA' 'BADAJOZ' 'BALEARS, ILLES' 'TOLEDO' 'LEON' 'ALICANTE'\n",
      " 'LERIDA' 'GRANADA' 'OURENSE' 'AVILA' 'SALAMANCA' 'TERUEL' nan 'BARCELONA'\n",
      " 'CANTABRIA' 'CACERES' 'TARRAGONA' 'PALENCIA' 'PALMAS, LAS' 'BURGOS'\n",
      " 'CIUDAD REAL' 'HUESCA' 'NAVARRA' 'SANTA CRUZ DE TENERIFE' 'GIPUZKOA'\n",
      " 'MELILLA' 'ALAVA' 'GUADALAJARA' 'RIOJA, LA' 'SORIA' 'ALMERIA' 'GIRONA'\n",
      " 'VALLADOLID' 'CUENCA' 'ZAMORA' 'BIZKAIA' 'CEUTA' 'SEGOVIA']\n",
      "NA values: 49330\n",
      "active_cust variables: [' 0' ' 1' nan]\n",
      "NA values: 1861\n",
      "income variables: ['160900.95' '74693.67' '35053.770000000004' ... '63867.66' '34341.18'\n",
      " '89018.37']\n",
      "NA values: 2240788\n",
      "segment variables: ['02 - PARTICULARES' '03 - UNIVERSITARIO' nan '01 - TOP']\n",
      "NA values: 163256\n",
      "savings_acct variables: ['0' '1']\n",
      "NA values: 0\n",
      "guarantees variables: ['0' '1']\n",
      "NA values: 0\n",
      "current_acct variables: ['1' '0']\n",
      "NA values: 0\n",
      "derivada_acct variables: ['0' '1']\n",
      "NA values: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14116\\483060620.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlist_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrename_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclean\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_col\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mf\"{clean} variables: {na_check[clean].unique()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"NA values: {na_check[clean].isna().sum()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MARIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "list_col = list(rename_col.values())\n",
    "\n",
    "for clean in list_col:\n",
    "    print (f\"{clean} variables: {na_check[clean].unique()}\")\n",
    "    print(f\"NA values: {na_check[clean].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.drop(['province_code', 'address_type', 'employee_spouse'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the previous output, we can see that the address_type column has only one value across the whole database, which is '1' (and null), so the column will be irrelevant to any future modeling. The column province_code has the same information as province_name, so we will drop the code one and keep the names. Lastly, the column employee_spouse has too many null values -over 10M, so we will drop it because it does not make sense to fill in those values since it is most of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MARIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_collection.py:4192: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('sex_H', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "c:\\Users\\MARIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_collection.py:4192: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('residency_spain', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "c:\\Users\\MARIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_collection.py:4192: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('birth_spain', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "c:\\Users\\MARIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_collection.py:4192: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('deceased', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "filtered_data = filtered_data.loc[filtered_data['sex_H'].notnull()]\n",
    "\n",
    "other = ['join_channel', 'province_name']\n",
    "filtered_data[other] = filtered_data[other].fillna('other')\n",
    "\n",
    "filtered_data['sex_H'] = filtered_data['sex_H'].map({'H': 1, 'V': 0}).fillna(0)\n",
    "\n",
    "columns_to_dummy = ['residency_spain', 'birth_spain', 'deceased']\n",
    "for col in columns_to_dummy:\n",
    "    filtered_data[col] = filtered_data[col].map({'S': 1, 'N': 0}).fillna(0)\n",
    "\n",
    "trim = ['customer_code', 'age', 'new_cust', 'seniority_in_months', 'primary_cust']\n",
    "for col in trim:\n",
    "    filtered_data[col] = filtered_data[col].astype(str).str.strip()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we are starting the cleaning process of the dataset. First we dropped the null values on the sex columns. We see the number 1861 repeating a lot across columns, so we will drop the null in sex and check if the other nulls will be drop. Since those nulls are across many columns, we concluded it would be best to drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filled null in columns join_channel and province_name with 'other' since they are string variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transformed the columns sex, residency_spain, birth_spain and deceased to dummy variables and filled na with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we trimmed the cells on columns customer_code, age, new_cust, seniority_in_months, and primary_cust for cleanliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = ['savings_acct', 'guarantees', 'current_acct', 'derivada_acct', 'payroll_acct', \n",
    "            'junior_acct', 'mas_particular_acct', 'particular_acct', 'particular_plus_acct', \n",
    "            'short_term_depo', 'medium_term_depo', 'long_term_depo', 'e_acct', 'funds', \n",
    "            'mortgage', 'pension', 'loans', 'taxes', 'credit_card', 'securities', \n",
    "            'home_acct', 'payroll_acct', 'pensions_2', 'direct_debt']\n",
    "\n",
    "filtered_data = filtered_data.fillna('0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will fill the rest of na values with '0' so we can keep cleaning the data. Later we will go back to these values and determine if the best approach is to fill it with '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_mapping = {\n",
    "    'customer_code': 'int',\n",
    "    'employee_index': 'str',\n",
    "    'country': 'str',\n",
    "    'sex_H': 'str',\n",
    "    'age': 'int',\n",
    "    'first_contract_date': 'datetime64[ns]',\n",
    "    'new_cust': 'int',\n",
    "    'seniority_in_months': 'int',\n",
    "    'primary_cust': 'int',\n",
    "    'last_date_primary': 'object',\n",
    "    'cust_type': 'object',\n",
    "    'cust_relationship': 'str',\n",
    "    'residency_spain': 'str',\n",
    "    'birth_spain': 'str',\n",
    "    'join_channel': 'str',\n",
    "    'deceased': 'str',\n",
    "    'province_name': 'str',\n",
    "    'active_cust': 'int',\n",
    "    'income': 'float',\n",
    "    'segment': 'object',\n",
    "    'savings_acct': 'int',\n",
    "    'guarantees': 'int',\n",
    "    'current_acct': 'int',\n",
    "    'derivada_acct': 'int',\n",
    "    'payroll_acct': 'int',\n",
    "    'junior_acct': 'int',\n",
    "    'mas_particular_acct': 'int',\n",
    "    'particular_acct': 'int',\n",
    "    'particular_plus_acct': 'int',\n",
    "    'short_term_depo': 'int',\n",
    "    'medium_term_depo': 'int',\n",
    "    'long_term_depo': 'int',\n",
    "    'e_acct': 'int',\n",
    "    'funds': 'int',\n",
    "    'mortgage': 'int',\n",
    "    'pension': 'int',\n",
    "    'loans': 'int',\n",
    "    'taxes': 'int',\n",
    "    'credit_card': 'int',\n",
    "    'securities': 'int',\n",
    "    'home_acct': 'int',\n",
    "    'payroll_acct': 'int',\n",
    "    'pensions_2': 'int',\n",
    "    'direct_debt': 'int'\n",
    "}\n",
    "filtered_data = filtered_data.astype(dtype_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tranform the dtypes across the whole dataset and call compute() again to force all the above changes across the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data to a CSV file\n",
    "filtered_data.to_csv('train_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10501007, 45)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10501007 entries, 27973 to 391600\n",
      "Data columns (total 45 columns):\n",
      " #   Column                Dtype         \n",
      "---  ------                -----         \n",
      " 0   date                  datetime64[ns]\n",
      " 1   customer_code         int32         \n",
      " 2   employee_index        object        \n",
      " 3   country               object        \n",
      " 4   sex_H                 object        \n",
      " 5   age                   int32         \n",
      " 6   first_contract_date   datetime64[ns]\n",
      " 7   new_cust              int32         \n",
      " 8   seniority_in_months   int32         \n",
      " 9   primary_cust          int32         \n",
      " 10  last_date_primary     object        \n",
      " 11  cust_type             object        \n",
      " 12  cust_relationship     object        \n",
      " 13  residency_spain       object        \n",
      " 14  birth_spain           object        \n",
      " 15  join_channel          object        \n",
      " 16  deceased              object        \n",
      " 17  province_name         object        \n",
      " 18  active_cust           int32         \n",
      " 19  income                float64       \n",
      " 20  segment               object        \n",
      " 21  savings_acct          int32         \n",
      " 22  guarantees            int32         \n",
      " 23  current_acct          int32         \n",
      " 24  derivada_acct         int32         \n",
      " 25  payroll_acct          int32         \n",
      " 26  junior_acct           int32         \n",
      " 27  mas_particular_acct   int32         \n",
      " 28  particular_acct       int32         \n",
      " 29  particular_plus_acct  int32         \n",
      " 30  short_term_depo       int32         \n",
      " 31  medium_term_depo      int32         \n",
      " 32  long_term_depo        int32         \n",
      " 33  e_acct                int32         \n",
      " 34  funds                 int32         \n",
      " 35  mortgage              int32         \n",
      " 36  pension               int32         \n",
      " 37  loans                 int32         \n",
      " 38  taxes                 int32         \n",
      " 39  credit_card           int32         \n",
      " 40  securities            int32         \n",
      " 41  home_acct             int32         \n",
      " 42  payroll_acct          int32         \n",
      " 43  pensions_2            int32         \n",
      " 44  direct_debt           int32         \n",
      "dtypes: datetime64[ns](2), float64(1), int32(30), object(12)\n",
      "memory usage: 2.4+ GB\n"
     ]
    }
   ],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
